{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "361d55d5-3be3-485a-8ebc-39162d574176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customers.csv columns: ['id', 'name', 'email', 'joined_at']\n",
      "orders.csv columns: ['id', 'customer_id', 'product_id', 'quantity', 'order_date']\n",
      "products.csv columns: ['id', 'name', 'category', 'price']\n",
      "CSVs preprocessed successfully.\n",
      "Loading CSV files from: /Users/olgacarrasco/airflow/dags\n",
      "Customers columns: ['id', 'name', 'email', 'joined_at']\n",
      "Products columns: ['id', 'name', 'category', 'price']\n",
      "Orders columns: ['id', 'customer_id', 'product_id', 'quantity', 'order_date']\n",
      "Connecting to DB at aws-0-eu-north-1.pooler.supabase.com:6543/postgres with user postgres.qmbbrplssfblgvghpczc\n",
      "âœ… ETL pipeline executed successfully.\n",
      "Validation failed: invalid literal for int() with base 10: 'please_enter_your_supabase_port'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'please_enter_your_supabase_port'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 280\u001b[0m\n\u001b[1;32m    277\u001b[0m execute_etl()\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# Step 3: Validate data\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m validate_data()\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# Step 4: Print Airflow setup instructions\u001b[39;00m\n\u001b[1;32m    283\u001b[0m print_airflow_setup()\n",
      "Cell \u001b[0;32mIn[3], line 121\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_data\u001b[39m():\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;66;03m# Connect to Supabase PostgreSQL\u001b[39;00m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;66;03m# Replace with your own Supabase credentials\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m         engine \u001b[38;5;241m=\u001b[39m create_engine(\n\u001b[1;32m    122\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostgresql+psycopg2://please_enter_your_supabase_user:please_enter_your_supabase_password@please_enter_your_supabase_host:please_enter_your_supabase_port/please_enter_your_supabase_dbname\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         )\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;66;03m# Check customers table\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         query_customers \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT id, name, email, joined_at FROM customers LIMIT 5;\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36mcreate_engine\u001b[0;34m(url, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sqlalchemy/util/deprecations.py:375\u001b[0m, in \u001b[0;36mdeprecated_params.<locals>.decorate.<locals>.warned\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    369\u001b[0m         _warn_with_version(\n\u001b[1;32m    370\u001b[0m             messages[m],\n\u001b[1;32m    371\u001b[0m             versions[m],\n\u001b[1;32m    372\u001b[0m             version_warnings[m],\n\u001b[1;32m    373\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    374\u001b[0m         )\n\u001b[0;32m--> 375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sqlalchemy/engine/create.py:514\u001b[0m, in \u001b[0;36mcreate_engine\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty_in_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m# create url.URL object\u001b[39;00m\n\u001b[0;32m--> 514\u001b[0m u \u001b[38;5;241m=\u001b[39m _url\u001b[38;5;241m.\u001b[39mmake_url(url)\n\u001b[1;32m    516\u001b[0m u, plugins, kwargs \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39m_instantiate_plugins(kwargs)\n\u001b[1;32m    518\u001b[0m entrypoint \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39m_get_entrypoint()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sqlalchemy/engine/url.py:738\u001b[0m, in \u001b[0;36mmake_url\u001b[0;34m(name_or_url)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given a string or unicode instance, produce a new URL instance.\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \n\u001b[1;32m    727\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    734\u001b[0m \n\u001b[1;32m    735\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name_or_url, util\u001b[38;5;241m.\u001b[39mstring_types):\n\u001b[0;32m--> 738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_url(name_or_url)\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m name_or_url\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sqlalchemy/engine/url.py:794\u001b[0m, in \u001b[0;36m_parse_url\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    791\u001b[0m     name \u001b[38;5;241m=\u001b[39m components\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m components[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mport\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 794\u001b[0m         components[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mport\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(components[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mport\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m URL\u001b[38;5;241m.\u001b[39mcreate(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomponents)\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'please_enter_your_supabase_port'"
     ]
    }
   ],
   "source": [
    "# etl_demo.ipynb\n",
    "# E-commerce ETL Pipeline Demo\n",
    "# This notebook tests the ETL pipeline for loading data into a Supabase PostgreSQL database\n",
    "# and prepares for automation with Apache Airflow.\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Add path to Airflow DAGs folder to access etl_utils\n",
    "dags_dir = '/Users/olgacarrasco/airflow/dags'\n",
    "sys.path.append(dags_dir)\n",
    "\n",
    "# Check if etl_utils.py exists in the DAGs folder\n",
    "if not os.path.exists(os.path.join(dags_dir, 'etl_utils.py')):\n",
    "    raise FileNotFoundError(\n",
    "        f\"etl_utils.py not found in {dags_dir}. \"\n",
    "        f\"Please ensure etl_utils.py is in the Airflow DAGs folder ({dags_dir}).\"\n",
    "    )\n",
    "\n",
    "# Import run_etl from etl_utils.py\n",
    "from etl_utils import run_etl\n",
    "\n",
    "# Section 1: Check and Preprocess CSVs\n",
    "def check_csv_paths():\n",
    "    # CSV paths in the Airflow DAGs folder\n",
    "    csv_paths = {\n",
    "        'customers': os.path.join(dags_dir, 'customers.csv'),\n",
    "        'products': os.path.join(dags_dir, 'products.csv'),\n",
    "        'orders': os.path.join(dags_dir, 'orders.csv')\n",
    "    }\n",
    "    \n",
    "    # Check if files exist\n",
    "    missing_files = []\n",
    "    for key, path in csv_paths.items():\n",
    "        if not os.path.exists(path):\n",
    "            missing_files.append(path)\n",
    "    \n",
    "    if missing_files:\n",
    "        print(\"Error: The following CSV files were not found in the Airflow DAGs folder:\")\n",
    "        for path in missing_files:\n",
    "            print(f\" - {path}\")\n",
    "        print(\"\\nPossible solutions:\")\n",
    "        print(f\"1. Ensure the CSV files (customers.csv, products.csv, orders.csv) are in {dags_dir}.\")\n",
    "        print(\"2. Check if the files have different names (e.g., 'Customers.csv' instead of 'customers.csv').\")\n",
    "        print(\"3. Move the CSV files to the Airflow DAGs folder:\")\n",
    "        print(f\"   mv /path/to/customers.csv {dags_dir}/customers.csv\")\n",
    "        raise FileNotFoundError(f\"Missing CSV files: {', '.join(missing_files)}\")\n",
    "    \n",
    "    return csv_paths\n",
    "\n",
    "def inspect_and_preprocess_csvs():\n",
    "    try:\n",
    "        # Check CSV paths\n",
    "        csv_paths = check_csv_paths()\n",
    "        \n",
    "        # Load CSVs\n",
    "        customers_df = pd.read_csv(csv_paths['customers'])\n",
    "        products_df = pd.read_csv(csv_paths['products'])\n",
    "        orders_df = pd.read_csv(csv_paths['orders'])\n",
    "        \n",
    "        # Inspect column names\n",
    "        print(\"customers.csv columns:\", customers_df.columns.tolist())\n",
    "        print(\"orders.csv columns:\", orders_df.columns.tolist())\n",
    "        print(\"products.csv columns:\", products_df.columns.tolist())\n",
    "        \n",
    "        # Rename 'customer_id' to 'id' in customers.csv to match etl_utils.py's merge\n",
    "        if 'customer_id' in customers_df.columns and 'id' not in customers_df.columns:\n",
    "            customers_df = customers_df.rename(columns={'customer_id': 'id'})\n",
    "            customers_df.to_csv(csv_paths['customers'], index=False)\n",
    "            print(f\"Renamed 'customer_id' to 'id' in {csv_paths['customers']} to match etl_utils.py\")\n",
    "        \n",
    "        # Check and rename columns in orders.csv if necessary\n",
    "        # 'id' is order_id (primary key), so keep it as 'id'\n",
    "        if 'cust_id' in orders_df.columns and 'customer_id' not in orders_df.columns:\n",
    "            orders_df = orders_df.rename(columns={'cust_id': 'customer_id'})\n",
    "            orders_df.to_csv(csv_paths['orders'], index=False)\n",
    "            print(f\"Renamed 'cust_id' to 'customer_id' in {csv_paths['orders']}\")\n",
    "        if 'prod_id' in orders_df.columns and 'product_id' not in orders_df.columns:\n",
    "            orders_df = orders_df.rename(columns={'prod_id': 'product_id'})\n",
    "            orders_df.to_csv(csv_paths['orders'], index=False)\n",
    "            print(f\"Renamed 'prod_id' to 'product_id' in {csv_paths['orders']}\")\n",
    "        \n",
    "        # Verify expected columns\n",
    "        expected_customers_cols = ['id', 'name', 'email', 'joined_at']  # Expect 'id' for etl_utils.py\n",
    "        expected_orders_cols = ['id', 'customer_id', 'product_id', 'quantity', 'order_date']\n",
    "        expected_products_cols = ['id', 'name', 'category', 'price']\n",
    "        \n",
    "        if not all(col in customers_df.columns for col in expected_customers_cols):\n",
    "            raise ValueError(f\"{csv_paths['customers']} missing expected columns: {expected_customers_cols}\")\n",
    "        if not all(col in orders_df.columns for col in expected_orders_cols):\n",
    "            raise ValueError(f\"{csv_paths['orders']} missing expected columns: {expected_orders_cols}\")\n",
    "        if not all(col in products_df.columns for col in expected_products_cols):\n",
    "            raise ValueError(f\"{csv_paths['products']} missing expected columns: {expected_products_cols}\")\n",
    "        \n",
    "        print(\"CSVs preprocessed successfully.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: CSV file not found - {str(e)}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error during CSV preprocessing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Section 2: Run ETL Pipeline\n",
    "def execute_etl():\n",
    "    try:\n",
    "        # Run the ETL pipeline from etl_utils.py\n",
    "        run_etl()\n",
    "        print(\"âœ… ETL pipeline executed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ETL pipeline failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Section 3: Validate ETL Output\n",
    "def validate_data():\n",
    "    try:\n",
    "        # Connect to Supabase PostgreSQL\n",
    "        # Replace with your own Supabase credentials\n",
    "        engine = create_engine(\n",
    "            \"postgresql+psycopg2://please_enter_your_supabase_user:please_enter_your_supabase_password@please_enter_your_supabase_host:please_enter_your_supabase_port/please_enter_your_supabase_dbname\"\n",
    "        )\n",
    "        \n",
    "        # Check customers table\n",
    "        query_customers = \"SELECT id, name, email, joined_at FROM customers LIMIT 5;\"\n",
    "        df_customers = pd.read_sql(query_customers, engine)\n",
    "        print(\"\\nCustomers Table (Top 5):\")\n",
    "        print(df_customers)\n",
    "        \n",
    "        # Check top 5 products by quantity sold\n",
    "        query_products = \"\"\"\n",
    "        SELECT product_id, SUM(quantity) AS total_sold\n",
    "        FROM orders\n",
    "        GROUP BY product_id\n",
    "        ORDER BY total_sold DESC\n",
    "        LIMIT 5;\n",
    "        \"\"\"\n",
    "        df_products = pd.read_sql(query_products, engine)\n",
    "        print(\"\\nTop 5 Products by Quantity Sold:\")\n",
    "        print(df_products)\n",
    "        \n",
    "        # Check orders table (explicitly select expected columns)\n",
    "        query_orders = \"\"\"\n",
    "        SELECT id, customer_id, product_id, quantity, order_date\n",
    "        FROM orders\n",
    "        LIMIT 5;\n",
    "        \"\"\"\n",
    "        df_orders = pd.read_sql(query_orders, engine)\n",
    "        print(\"\\nOrders Table (Top 5):\")\n",
    "        print(df_orders)\n",
    "    except Exception as e:\n",
    "        print(f\"Validation failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Section 4: Define ETL Functions (for reference/testing, not used in production)\n",
    "# These mirror etl_utils.py but are included for notebook testing\n",
    "def transform_customers(df):\n",
    "    import re\n",
    "    df['email'] = df['email'].str.strip().str.lower()\n",
    "    email_pattern = r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$'\n",
    "    df = df[df['email'].apply(lambda x: bool(re.match(email_pattern, x)))]\n",
    "    df['name'] = df['name'].str.strip().str.title()\n",
    "    df['joined_at'] = pd.to_datetime(df['joined_at'], errors='coerce')\n",
    "    df = df.dropna(subset=['joined_at'])\n",
    "    df = df.drop_duplicates(subset=['id'])  # Use 'id' to match etl_utils.py\n",
    "    return df\n",
    "\n",
    "def transform_products(df):\n",
    "    df['name'] = df['name'].str.strip().str.title()\n",
    "    df['category'] = df['category'].str.strip().str.title()\n",
    "    df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "    df = df[df['price'] >= 0].dropna(subset=['price'])\n",
    "    df = df.drop_duplicates(subset=['id'])\n",
    "    return df\n",
    "\n",
    "def transform_orders(df, customers_df, products_df):\n",
    "    df['quantity'] = pd.to_numeric(df['quantity'], errors='coerce').fillna(0).astype(int)\n",
    "    df = df[df['quantity'] > 0]\n",
    "    df['order_date'] = pd.to_datetime(df['order_date'], errors='coerce')\n",
    "    df = df.dropna(subset=['order_date'])\n",
    "    valid_customers = set(customers_df['id'])  # Use 'id' to match customers.csv\n",
    "    valid_products = set(products_df['id'])\n",
    "    df = df[df['customer_id'].isin(valid_customers) & df['product_id'].isin(valid_products)]\n",
    "    df = df.drop_duplicates(subset=['id'])  # 'id' is order_id\n",
    "    return df\n",
    "\n",
    "def insert_customers(df, conn):\n",
    "    with conn.cursor() as cur:\n",
    "        for _, row in df.iterrows():\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO customers (id, name, email, joined_at)\n",
    "                VALUES (%s, %s, %s, %s)\n",
    "                ON CONFLICT (id) DO NOTHING;\n",
    "            \"\"\", (row['id'], row['name'], row['email'], row['joined_at']))\n",
    "    conn.commit()\n",
    "\n",
    "def insert_products(df, conn):\n",
    "    with conn.cursor() as cur:\n",
    "        for _, row in df.iterrows():\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO products (id, name, category, price)\n",
    "                VALUES (%s, %s, %s, %s)\n",
    "                ON CONFLICT (id) DO NOTHING;\n",
    "            \"\"\", (row['id'], row['name'], row['category'], row['price']))\n",
    "    conn.commit()\n",
    "\n",
    "def insert_orders(df, conn):\n",
    "    with conn.cursor() as cur:\n",
    "        for _, row in df.iterrows():\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO orders (id, customer_id, product_id, quantity, order_date)\n",
    "                VALUES (%s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (id) DO NOTHING;\n",
    "            \"\"\", (row['id'], row['customer_id'], row['product_id'], row['quantity'], row['order_date']))\n",
    "    conn.commit()\n",
    "\n",
    "# Section 5: Manual ETL for Testing (Optional)\n",
    "def run_manual_etl():\n",
    "    try:\n",
    "        # Connect to database\n",
    "        # Replace with your own Supabase credentials\n",
    "        conn = psycopg2.connect(\n",
    "            host=\"please_enter_your_supabase_host\",\n",
    "            port=\"please_enter_your_supabase_port\",\n",
    "            user=\"please_enter_your_supabase_user\",\n",
    "            password=\"please_enter_your_supabase_password\",\n",
    "            dbname=\"please_enter_your_supabase_dbname\"\n",
    "        )\n",
    "        \n",
    "        # Load and preprocess CSVs\n",
    "        csv_paths = check_csv_paths()\n",
    "        customers_df = pd.read_csv(csv_paths['customers'])\n",
    "        products_df = pd.read_csv(csv_paths['products'])\n",
    "        orders_df = pd.read_csv(csv_paths['orders'])\n",
    "        \n",
    "        # Apply transformations\n",
    "        customers_df = transform_customers(customers_df)\n",
    "        products_df = transform_products(products_df)\n",
    "        orders_df = transform_orders(orders_df, customers_df, products_df)\n",
    "        \n",
    "        # Insert data\n",
    "        insert_customers(customers_df, conn)\n",
    "        insert_products(products_df, conn)\n",
    "        insert_orders(orders_df, conn)\n",
    "        \n",
    "        # Close connection\n",
    "        conn.close()\n",
    "        print(\"âœ… Manual ETL ran successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Manual ETL failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Section 6: Airflow Setup Instructions\n",
    "def print_airflow_setup():\n",
    "    print(\"\\nRun the following commands in your terminal to set up Airflow:\")\n",
    "    print(\"1. Install Airflow:\")\n",
    "    print(\"   pip install apache-airflow\")\n",
    "    print(\"2. Initialize Airflow database:\")\n",
    "    print(\"   airflow db migrate\")\n",
    "    print(\"3. Create an admin user:\")\n",
    "    print(\"   airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com\")\n",
    "    print(\"4. Start the Airflow API server:\")\n",
    "    print(\"   airflow api-server --port 8080\")\n",
    "    print(\"5. Start the Airflow scheduler (in a separate terminal):\")\n",
    "    print(\"   airflow scheduler\")\n",
    "    print(\"Alternatively, run Airflow in standalone mode for testing:\")\n",
    "    print(\"   airflow standalone\")\n",
    "    print(\"\\nAdd etl_dag.py to your Airflow DAGs folder and trigger the 'csv_to_postgres_etl' DAG.\")\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Inspect and preprocess CSVs\n",
    "    inspect_and_preprocess_csvs()\n",
    "    \n",
    "    # Step 2: Run ETL pipeline (uses etl_utils.py)\n",
    "    execute_etl()\n",
    "    \n",
    "    # Step 3: Validate data\n",
    "    validate_data()\n",
    "    \n",
    "    # Step 4: Print Airflow setup instructions\n",
    "    print_airflow_setup()\n",
    "    \n",
    "    # Optional: Run manual ETL for testing\n",
    "    # run_manual_etl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0277c861-f55f-4783-84bd-c7a2c3a2713a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
